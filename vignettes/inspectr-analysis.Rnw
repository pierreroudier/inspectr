\documentclass{article}
% \VignetteIndexEntry{Analysing spectroscopy data using inspectr}

\usepackage{graphicx}
\usepackage[colorlinks=true,urlcolor=blue]{hyperref}
\usepackage{color}
\usepackage{subfig}
\usepackage{multirow}


% \VignetteDepends{sp,ggplot2}


\setkeys{Gin}{width=0.5\textwidth}

\title{Analysing spectroscopy data using {\tt inspectr}}

\author{
Pierre Roudier\footnote{Landcare Research -- Manaaki Whenua, Private Bag 11052, Manawatu Mail Centre, Palmerston North 4442, New Zealand.
{\tt roudierp@landcareresearch.co.nz}}
}

\date{\today}

\begin{document}

\maketitle
\tableofcontents

<<load_deps, echo=FALSE, message=FALSE>>=
library(caret)
library(ggplot2)
library(inspectr)

load_oz <- function(){
  data(australia)
  spectra(australia) <- sr_no ~ ... ~ 350:2500
  australia
}
@

\section{Data screening}

For this tutorial, the {\tt australia} dataset will be used. First, load the {\tt inspectr} library. The {\tt australia} data can then be accessed and converted into a {\tt SpectraDataFrame} object using the {\tt spectra} function. The {\tt big.head} and {\tt big.tail} functions also help to screen spectra datasets that may have a huge number of columns.
<<load_oz, echo=TRUE, eval=TRUE, cache=FALSE>>=
data(australia)
big.head(australia)
spectra(australia) <- sr_no ~ ... ~ 350:2500
@

\subsection{Data overview}

A first task in analysing a spectral dataset is to screen the daatset using simple statistical indicators and data visualisation techniques. Two parts of the spectra dataset can be explored in parallel, i.e. the spectra and the attribute data (e.g. lab measurements) that may be associated with some of each of these spectra.

Note that general information about the {\tt SpectraDataFrame} object can be accessed using the {\tt summary} function:
<<summary>>=
summary(australia)
dim(australia)
@

\subsubsection{Accessing attribute data}
The {\tt SpectraDataFrame} class allows the user to asociate attribute data to a collection of spectra. The attribute data is stored as a {\tt data.frame} object in the {\tt SpectraDataFrame} object, and can be accessed using the {\tt features} function:
<<features>>=
# Getting the attribute data
oz_data <- features(australia)
class(oz_data)
summary(oz_data)
@
More statistics (for example standard deviation as given by the {\tt sd} function) can be accessed using the usual {\tt data.frame} manipulation methods. Moreover, the \${} operator can be used directly on the {\tt SpectraDataFrame} object to access attributes.
<<simple_df>>=
# Standard deviation
sd(australia$ph)
sd(oz_data$ph)
apply(oz_data, 2, sd)

# Correlation between attributes
cor(features(australia))
@
In particular, the skewness of the attribute data can be interesting to support the decision to log-transform the attribute data prior to modeling. Skewness can be computed yusing the {\tt skewness} function from the {\tt e1071} package:
<<skewness, message=FALSE>>=
library(e1071)
apply(oz_data, 2, skewness)
@
In this instance, carbon may be log-transformed prior to modeling: 
<<skewness_carbon>>=
skewness(australia$carbon)
skewness(log1p(australia$carbon))
@

\subsubsection{Modify and create attribute data}
\label{sec:modify_attributes}

Attribute data can be modified or created like you would do for a {\tt data.frame} object, using {\tt \${}} or {\tt `[[`}:
<<set_attribute>>=
australia$log_carbon <- log1p(australia$carbon)
australia[['letters']] <- rep(LETTERS[1:3], length.out = nrow(australia))
summary(australia)
@

<<remove_letters, echo=FALSE>>=
australia$log_carbon <- australia$letters <- NULL
@

The {\tt mutate} function is analoguous to the {\tt mutate} function in the {\tt plyr} package. It provides a easy way to add a suite of new attributes, or to replace existing ones. It is similar to the {\tt transform} function, but it executes the transformations iteratively so that later transformations can use the columns created by earlier transformations.
<<mutate_attribute>>=
australia <- mutate(australia, log_carbon = log1p(carbon))
summary(australia)
@
An additional bonus of the {\tt mutate} function is that it can not only create or modify one or several attributes on one command, but it can also transform the spectra matrix at the same time. This will be address in the \ref{sec:transformations} section.

<<remove_log_carbon, echo=FALSE>>=
australia$log_carbon <- NULL
@

\subsubsection{Visualising attribute data}
Graphical visualisation of attribute data can help detecting colinearity between attributes. This task is again made easy using the {\tt features} function, which returns a {\tt data.frame} containing the attribute data. From then on, the classic graphical capabilities of {\tt R} can be exploited. For example, a scatterplot of the attribute data can be generated using the {\tt pairs} function:

<<simple_plot_attribute, echo=TRUE, eval=TRUE, cache=TRUE, fig.height=4, fig.width=4>>=
pairs(features(australia))
@

Other graphics libraries, such as {\tt ggplot2}, can also be used to visualise attribute data. The following example is generating a probability density plot for the attribute data:

<<ggplot_attribute, cache=TRUE, message=FALSE, fig.height=5, fig.width=10>>=
library(reshape2)
library(ggplot2)

m <- melt(features(australia))
ggplot(m) + 
  geom_density(aes(x=value, fill=variable)) +
  facet_wrap(~variable, scales = "free")
@

\subsubsection{Principal components analysis}

A principal component analysis (PCA) is a very standard proceudre when working with spectral data. It is a very useful way to take a different view at your data, and reduce the often overwhelming dimensionality of your spectral dataset.

The {\tt prcomp} function is offering PCA capabilities in base {\tt R}. The matrix of the spectra of a given {\tt Spectra*} object can be extracted using the {\tt spectra} function and passed to {\tt prcomp} to do a PCA analysis of the spectra collection:

<<pca, fig.width=4, fig.height=4>>=
pca <- prcomp(spectra(australia), scale. = TRUE, center=TRUE)
plot(pca)
@

Plotting the proportion of variance of each principal component can help decide how many principal components to retain for further analysis. The rotated data can be extracted from the PCA result. In the following example we will select the three first principal components:

<<pca_rotated, fig.width=4, fig.height=4>>=
rot_data <- data.frame(pca$x[, 1:3])
head(rot_data)
plot(rot_data)
@

\subsection{Detecting outliers}

The PCA results can be used to identify potential outliers in the dataset, and to exclude these outliers before we do any further analysis.

\section{Pre-processing}

\subsection{Separating {\tt SpectraDataFrame} object according to a given factor}

% split (+ lapply)

If we add a dummy factor called ``\verb|soil_horizon|'' to our {\tt SpectraDataFrame} object, we can split that object according to that factor using the {\tt split} function. This will return a list of {\tt SpectraDataFrame} objects --- with one {\tt SpectraDataFrame} for each \verb|soil_horizon| level. This facilitates the use of the {\tt lapply} function for repetitive tasks (in the following example we simply retrieve the number of samples in each \verb|soil_horizon| level).

<<split_sdf, fig.width=3, fig.height=3>>=
australia$soil_horizon <- sample(c('A', 'Bt', 'Bw', 'Cr'), size = nrow(australia), replace = TRUE)
oz <- split(australia, 'soil_horizon')
# Apply a given function on each group of spectra
lapply(oz, nrow)
# Plot a specific group of spectra
plot(oz$Bw)
@

Note that lists of {\tt Spectra*} objects can be concatenated back together uisng the {\tt rbind} function:
<<rbind>>=
summary(do.call("rbind", oz))
@

<<cleanup, echo=FALSE>>=
australia$soil_horizon <- NULL
rm(oz)
@

\subsection{Transformations} 
\label{sec:transformations}
% log(1/R)
The transformations on the spectral values contained in any {\tt Spectra*} object is possible using the  \verb|apply_spectra| function, which is very similar to the {\tt *apply} family of function in base {\tt R}. For example, a common transformation is to convert reflectance values into absorbance values using the $log(\frac{1}{R})$ equation:
<<to_absorbance_apply, fig.width=3.5, fig.height=3.5>>=
# Apply the log(1/R) function to the spectra matrix
oz_abs <- apply_spectra(australia, function(nir) log(1/nir))
# Plot only the 10 first spectra
plot(oz_abs[1:10,], col = "black", ylab = "Absorbance")
@
As seen previously (section \ref{sec:modify_attributes}), the {\tt mutate} function is very useful when working on attributes. It can additionally be used to affect the spectra matrix as wll. In this case, the reserved variable name {\tt nir} should be used in the function declaration inside the {\tt mutate} function. This way, both the attributes creation/transformations and the spectra transformation can be done in a one-liner:
<<to_absorbance_mutate, fig.width=3.5, fig.height=3.5, message=FALSE>>=
oz_abs <- mutate(australia, nir = log(1/nir), log_carbon = log1p(carbon))
@
Obviously, any function can be applied to the spectra. An illustration of this is the empirical Kubelka-Munk equation, which is defined as follows:
\begin{equation}
\label{eq:kubelka}
f(R) = \frac{(1 - R^{2})}{2 \times R}
\end{equation}
The \verb|apply_spectra| function can be used to apply the Kubelka-Munk equation to the spectra collection:
% Kubelka-Munk
<<kubelka, fig.width=3.5, fig.height=3.5>>=
# Define the Kubelka-Munk function
kubelka <- function(r) (1 - r^2)/(2*r)
# Apply the function to the spectra matrix
oz_km <- apply_spectra(australia, kubelka)
# Plot only the 10 first spectra
plot(oz_km[1:10,], col = "black", ylab = "Kubelka-Munk")
@

\subsection{Selecting or removing wavelengths}
% cut
<<select_wl, fig.width=3, fig.height=3>>=
oz_select <- cut(australia, wl = 980:1020)
plot(oz_select)

oz_remove <- cut(australia, wl = -1*980:1020)
plot(oz_remove)
@

\subsection{Interpolating the spectra}
% splice
Spectral data obtained from some spectrometers can sometimes be affected by steps in the data (Figure \ref{fig:splice}). These steps are usually located at the junction between the different sensors along the wavelength range. To overcome this problem, ASD is proposing a transformation called \textit{splicing} --- which is a local interpolation of the spectral data.

In {\tt inspectr}, this operation is possible using the {\tt splice} function:
<<splice>>=
oz_spliced <- splice(australia, wl = c(725:1020, 1801:1950))
@

The Figure \ref{fig:splice} compares the raw data with the spliced data.
\begin{figure}[h]
<<splice_figs, echo=FALSE, out.width="0.49\\textwidth", fig.width=4, fig.height=3, fig.show='hold'>>=  
plot(cut(australia, wl =980:1020)[1:10,])
plot(cut(oz_spliced, wl =980:1020)[1:10,])
@
\caption{The {\tt splice} function interpolates the spectra over a given wavelength range. (left) Before. (right) After.}
\label{fig:splice}
\end{figure}

\subsection{Aggregating the spectra}
% aggregate_spectra

<<aggregate_simple, fig.width=3, fig.height=3>>=
australia$soil_horizon <- sample(c('A', 'Bt', 'Bw', 'Cr'), size = nrow(australia), replace = TRUE)
oz_ag <- aggregate_spectra(australia, fun = mean)
plot(oz_ag)
@

<<aggregate_factor, fig.width=6, fig.height=3>>=
# Adding factor against which to aggregate data
australia$soil_horizon <- sample(c('A', 'Bt', 'Bw', 'Cr'), size = nrow(australia), replace = TRUE)
# Get median spectra for each soil horizon
oz_ag <- aggregate_spectra(australia, fun = mean, id = 'soil_horizon')
# Advanced plot using ggplot2
m_oz_ag <- melt_spectra(oz_ag, attr = "soil_horizon")
ggplot(m_oz_ag) + geom_line(aes(x=wl,y=nir,colour=soil_horizon)) + labs(x="Wavelength (nm)", y = "Reflectance") + scale_colour_discrete("Horizon")
@

\subsection{Resampling the spectra}
% bin

\subsection{Baseline}
% base_line
<<baseline>>=
# Correction using the default method (irls)
bl <- base_line(australia)
plot(bl)

# Specifying another method for baseline calculation
bl2 <- base_line(australia, method = "modpolyfit")
plot(bl2)
@

% continuum_removal

\subsection{Signal processing on the spectra}

\subsubsection{Derivation}

<<simple_derivation, fig.width=8, fig.height=6, message=FALSE>>=
# First derivative
oz_deriv <- apply_spectra(australia, diff, differences = 1)
plot(oz_deriv[1:4,])
@

\subsubsection{Smoothing and derivation using a Savitzky-Golay filter}
% savitzky-golay
<<sg, fig.width=8, fig.height=6, message=FALSE>>=
library(signal)
oz_sg <- apply_spectra(australia, sgolayfilt,  n = 11, p = 2, m = 1)
plot(oz_sg[1:4,])
@

\subsubsection{Removing noise}
% snv, rnv, msc

\subsubsection{Wavelets-based decomposition}
% wavelets
<<wavelets, cache=TRUE, fig.width=8, fig.height=6>>=
# Load library to compute wavelet decomposition
library(waveslim)
# Function extracting the de-noised spectra
wv_denoise <- function(nir, n.levels){
  res <- modwt(nir, n.levels=n.levels)
  res[[paste('s', n.levels, sep = '')]]
}
# Applying wavelet function on spectra
oz_wl <- apply_spectra(oz_sg, wv_denoise, n.levels = 5)
# Quick plot
plot(oz_wl[1:4,])
@

\subsection{External parameter orthogonalisation}
% epo

\section{Calibration of regression models}

\subsection{Separating calibration and validation sets}

\subsubsection{Random sampling}
% separate
<<separate, eval=TRUE, echo=TRUE>>=
set.seed(1)
sl <- separate(australia, calibration = 0.7)
@
The {\tt separate} function is returning a \emph{list} of {\tt Spectra*} objects, and is thus very well suited to be used in combination with {\tt lapply}:
<<separate_2, eval=TRUE, echo=TRUE>>=
names(sl)
lapply(sl, nrow)
lapply(sl, ids)
@

\subsubsection{Kennard-Stone sampling}
% kenstone
<<ks, cache=TRUE, fig.width=3, fig.height=3, tidy=TRUE, tidy.opts=(width.cutoff=50)>>=
# Get the indices of the selected samples
idx <- kenstone(australia, size = 10, progress = FALSE)
# Plotting the selected samples in principal components space
ggplot() +  
  geom_point(data = rot_data, aes(x = PC1, y = PC2)) +  
  geom_point(data = rot_data[idx,], aes(x = PC1, y = PC2), pch = 24, fill = "red", size = 3)  
@

\subsubsection{Latin Hypercube sampling}
% clhs::clhs
<<clhs, cache=TRUE, message=FALSE, fig.width=3, fig.height=3>>=
# Load the clhs library
library(clhs)
# Get the indices of the selected samples
idx <- clhs(rot_data, size = 10, progress=FALSE)
# Plotting the selected samples in principal components space
ggplot() +  
  geom_point(data = rot_data, aes(x = PC1, y = PC2)) +  
  geom_point(data = rot_data[idx,], aes(x = PC1, y = PC2), pch = 24, fill = "red", size = 3)  
@

\subsection{Regression}

\subsubsection{Using the {\tt caret} package}
% train

\subsubsection{Partial Least Squares Regression}

<<preprocessing, cache=TRUE, message=FALSE>>=
oz <- apply_spectra(australia, sgolayfilt,  n = 11, p = 2, m = 1)
oz$log_carbon = log1p(oz$carbon)
# Separate calibration and validation
idx <- kenstone(oz, size = 75, progress = FALSE)
oz_calib <- oz[idx,]
oz_valid <- oz[-idx,]
@

<<train_pls_config, cache=FALSE, message=FALSE>>=
library(caret)
## 10-fold CV
config_pls <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 2
)
@

<<train_pls_fit, cache=TRUE, warning=FALSE, message=FALSE>>=
fit_pls <- train(
      x = spectra(oz_calib),
      y = oz_calib$log_carbon,
      method = "pls",
      trControl = config_pls,
      tuneLength = 10
    )
@

<<train_pls_summary, cache=FALSE>>=
print(fit_pls)
plot(fit_pls)
@

\subsubsection{Regression rules}

<<train_cubist_config, cache=FALSE, message=FALSE>>=
## 5-fold CV
config_cubist <- trainControl(
  method = "cv",
  number = 5
)
grid_cubist <- expand.grid(.committees = 1:5, .neighbors = 1:3)
@

<<train_cubist_fit, cache=TRUE, warning=FALSE, message=FALSE>>=
fit_cubist <- train(
      x = spectra(oz_calib),
      y = oz_calib$log_carbon,
      method = "cubist",
      trControl = config_cubist,
      tuneGrid = grid_cubist
    )
@

<<train_cubist_summary, cache=FALSE>>=
print(fit_cubist)
plot(fit_cubist)
@

<<predict_cubist, cache=FALSE>>=
print(fit_cubist)
plot(fit_cubist)
@

<<cubist_validation>>=
preds <- predict(object = fit_cubist, newdata = spectra(oz_valid))
obs <- oz_valid$log_carbon
plot(preds ~ obs, data = data.frame(preds=preds, obs = obs), asp = 1)
@

\subsubsection{Comparing models}

<<compare_models, fig.height=4.5, fig.width=4.5, message=FALSE>>=
models <- list(PLS = fit_pls, Cubist = fit_cubist)
preds <- extractPrediction(models, testX = spectra(oz_valid), testY = oz_valid$log_carbon)
plotObsVsPred(preds)
@

\subsection{Classification}

\subsubsection{Unsupervised classification}

\paragraph{K-means classification}
% % k-means, clust, diana & co

\paragraph{Hierarchical classification}

\subsubsection{Supervised classification}

<<create_class, cache=TRUE>>=
australia <- load_oz()
soil_class <- kmeans(features(australia), centers=5)$cluster
soil_class <- factor(soil_class, levels = 1:5, labels = LETTERS[1:5])
australia$soil_class <- soil_class

oz <- apply_spectra(australia, sgolayfilt,  n = 11, p = 2, m = 1)

# Separate calibration and validation
oz_calib <- oz[idx,]
oz_valid <- oz[-idx,]
@

<<train_c5_config, cache=FALSE, message=FALSE>>=
## 10-fold CV
config_c5 <- trainControl(
  method = "cv",
  number = 10
)
@

<<train_c5_fit, cache=TRUE, warning=FALSE, message=FALSE>>=
fit_c5 <- train(
      x = spectra(oz_calib),
      y = oz_calib$soil_class,
      method = "C5.0Rules",
      trControl = config_c5
    )
@

<<train_c5_summary, cache=FALSE>>=
print(fit_c5)
@

% Networks

\section{Exploring models performance}
% some sort of performance monitoring function with RPD and RPIQ
% to plug into train(), and to use on preds vs observed data.frame

% Plotting results

\end{document}
